---
layout: post
title: "Story 1 - Resiliency in a distributed system."
date: 2017-02-20
---

# Payment Platform

![Architecture overview]({{ site.url }}/assets/architecture_overview.svg)

There is one very central point in the architecture of our payment platform, the Order Service. It orchestrates all payment flows, receiving user requests and making downstream calls to both internal and external services. Payment Provider Adapters (PPAs) are responsible for integrating with APIs of external payment providers (credit card operators, SMS payment providers, banks, etc.). PPAs act as an intermediary when Order Service wants to make an external call. The platform also provides a digital wallet service, enabling users to store money for future payments or withdrawal.

Depending on the payment flow, most user requests involve multiple synchronous calls to downstream services, often including more than one external provider. As we integrate with a lot of country-specific providers, quality of their APIs vary a lot. It is impossible to define a common [SLA][sla]{:target="_blank"} with all of them, so we cannot rely on any solid boundaries for request execution time or service uptime.

# The incident

In the beginning, when the payment platform was only receiving a moderate traffic, we could easily overcome minor hiccups of our external providers. But after months of steady increase in transaction volume, one such minor hiccup caused a major downtime of our platform. One of the providers was experiencing back-end issues, bumping up its API response times to tens of seconds, causing a lot of requests to return failure or simply time out on our side. While this was only one provider among many others, involved in just a part of payment flows, the situation led to exhaustion of client request pools on all Order Service instances. This basically rendered whole payment platform unusable, as it was unable to accept more client requests, also preventing execution of payment flows that exclusively involved healthy external providers. There was no quick way of alleviating the problem, except to wait for the external provider to recover.

![Exhausted client request pool]({{ site.url }}/assets/request_pool.svg)

# Finding a solution

After the incident, I was tasked with analysing the problem and exploring possible solutions.

The easiest would be to just scale up services involved, both horizontally and vertically. Switching to greater number of more powerful instances, and adjusting client request pool accordingly, would probably solve the problem to some extent. However, with a steadily increasing traffic, it would be just a matter of time before hitting the limits again.

Another solution I looked into was isolating points of access between external providers. Instead of using common thread pool for all requests to downstream services, introduce separate HTTP client for each service. This way, each HTTP client in Order Service would have its own thread pool and a custom timeout value, configured to match individual performance characteristics of a service it talks to. With a client request pool size set way above the sizes of those individual downstream pools, one misbehaving external provider would not be able to take down the whole platform. Reaching the limit of open connections to a particular provider would just cause all additional user requests involving this provider to fail immediately, consuming at most a fixed amount of threads in the client request pool.

# Hystrix

Further researching the subject, I have discovered [Hystrix][hystrix-homepage]{:target="_blank"}. It is a latency and fault tolerance library designed to isolate points of access to remote systems, stop cascading failure and enable resilience in distributed systems. It has been developed by [Netflix][netflix-github]{:target="_blank"} among many other great libraries supporting development of microservices, like [Eureka][eureka]{:target="_blank"}, [Ribbon][ribbon]{:target="_blank"} or [Asgard][asgard]{:target="_blank"}. Hystrix is able to provide the same thread pool isolation as mentioned above, together with lots of other useful features.

I stopped searching here, in my opinion, it was the right tool for the job.

# The solution

For Spring Boot applications, thanks to [javanica][javanica-github]{:target="_blank"} contrib library, enabling Hystrix boils down to including a few annotations.

{% highlight java %}
@Component
@DefaultProperties(groupKey = "Payex", threadPoolKey = "Payex")
public class Payex {

    Client client;

    @HystrixCommand(commandKey = "PayexGetUserAgreement",
                    fallbackMethod = "getUserAgreementFallback")
    public Response getUserAgreement(String userId) {
        return client.getUserAgreement(userId);
    }

    private Response getUserAgreementFallback(String userId) {
        return UserAgreement.empty(); // or throw exception here
    }

    @HystrixCommand(commandKey = "PayexPutUserAgreement")
    public Response putUserAgreement(UserAgreementRequest request) {
        return client.putUserAgreement(request);
    }
}
{% endhighlight %}

In the above example, `Payex` is a component responsible for communication with external payment provider called Payex. It supports 2 operations: `getUserAgreement` and `putUserAgreement`. Both methods are annotated with `@HystrixCommand`. Spring Cloud automatically wraps Spring beans with that annotation in a proxy that is connected to the Hystrix [circuit breaker][circuit-breaker]{:target="_blank"}. Implementation of this pattern is one the core features of Hystrix. 

Annotated method call is wrapped with a `HystrixCommand` object that executes within a separate thread pool designated specifically for communication with Payex API ([command pattern][command-pattern]{:target="_blank"}). Then, Hystrix can monitor executions of such calls, measuring successes, failures (exceptions thrown by client), timeouts, and thread rejections. Commands that exceed a configured execution timeout or throw an exception are stopped, and a fallback method is invoked instead, to provide a default return value or invoke some recovery logic (see `getUserAgreementFallback`). `HystrixCommand` calls without a specified fallback method return an exception in such cases (see `putUserAgreement`).

If the percentage of errors passes a certain threshold, circuit-breaker trips into an `open` state. It stays in this state for a configured amount of time, rejecting all further method calls and immediately routing to the fallback method. After this sleep window passes, circuit-breaker switches into a `half-open` state and lets a single request in. If it is successful, circuit-breaker goes back to the `closed` state, resuming regular communication with the external provider. 

Hystrix can be easily configured using [properties][hystrix-configuration], e.g. within an `application.yml` file:
```
hystrix:
    command:
        PayexGetUserAgreement:
            execution:
                isolation:
                    thread:
                        timeoutInMilliseconds: 1000
            circuitBreaker:
                requestVolumeThreshold: 20
                errorThresholdPercentage: 50
        PayexPutUserAgreement: ...
    threadpool:
        Payex:
            coreSize: 10
```

# Benefits

After benchmarking the solution, Hystrix proved to be really effective in solving all kinds of problems related to the incident described above. We have decided to use it with every HTTP client in Order Service. It completely isolates access points between different external payment providers and allows fine tuning of configuration for each of them. We can individually control timeouts, error thresholds, thread pool sizes, to match the performance characteristics of the provider's API. Regardless of a scale of failure, with Hystrix, one downstream service is not able to take down the whole application any more. Additionally, `open` circuit-breaker helps downstream services recover. Stopping a stream of requests to a service in trouble can potentially shorten its downtime, and minimise possible data integrity issues.

Hystrix also comes with a real-time metrics stream and a monitoring [dashboard][hystrix-dashboard]{:target="_blank"}. It offers us a great insight into system behaviour and performance characteristics of all integration points. After integrating the metrics with our Datadog instance, we receive monitor alerts on circuit-breaker state changes.

Screenshot below shows a dashboard for the `Payex` component, with one circuit-breaker tripped due to error percentage exceeding the threshold of 50%.

![Hystrix dashboard]({{ site.url }}/assets/hystrix_dashboard.png)

[sla]: https://en.wikipedia.org/wiki/Service-level_agreement
[hystrix-homepage]: https://github.com/Netflix/Hystrix
[netflix-github]: https://github.com/Netflix
[eureka]: https://github.com/Netflix/eureka
[asgard]: https://github.com/Netflix/asgard
[ribbon]: https://github.com/Netflix/ribbon
[spinnaker]: http://www.spinnaker.io/
[javanica-github]: https://github.com/Netflix/Hystrix/tree/master/hystrix-contrib/hystrix-javanica
[circuit-breaker]: https://martinfowler.com/bliki/CircuitBreaker.html
[command-pattern]: https://en.wikipedia.org/wiki/Command_pattern
[hystrix-dashboard]: https://github.com/Netflix/Hystrix/wiki/Dashboard
[hystrix-configuration]: https://github.com/Netflix/Hystrix/wiki/Configuration