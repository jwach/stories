---
layout: post
title: "Story 1 - Lesson on distributed system's resiliency."
date: 2017-02-17
---

# Payment Platform

[slice of architecture diagram]

There is one very central point in the architecture of our payment platform, the Order Service. It orchestrates all payment flows, receiving user requests and making downstream calls to payment provider adapters (PPAs)(don't write about it?? only mention as a minor hop) and other internal services. PPAs are responsible for integrating with APIs of external payment providers (credit cards, SMS payments, wire transfers, etc.). (we allow combined flows and offer internal e-wallet solution)

Depending on the flow, most user requests involve multiple synchronous calls to downstream services, often including more than one PPA. As we integrate with a lot of country-specific providers, quality of their APIs vary a lot. It is impossible to define a common [SLA][sla] with all of them, so we cannot rely on any solid boundaries for request execution time or service uptime. (mention also response times hitting 10s)

# The incident

In the beginning, when the payment platform was only receiving a moderate traffic, we could easily overcome minor hickups of our external providers. But after months of steady increase in transaction volume, one such minor hickup caused a major downtime of our platform. One of the providers was experiencing back-end issues, bumping up its API response times to tens of seconds, causing a lot of requests to return failure or simply time out on our side. While this was only one provider among many others, involed in just a part of payment flows, the situation led to exhaustion of client request pools on all Order Service instances. This basically rendered whole payment platform unusable, as it was unable to accept more client requests, also preventing execution of payment flows that exclusively involved healthy external providers. There was no quick way of alleviating the problem, except to wait for the external provider to recover.

[diagram with client request pool -> ppas]

# Finding a solution

After the incident, I was tasked with analysing the problem and exploring possible solutions. The easiest would be to just scale up services involved, both horizontally and vertically. Switching to greater number of more powerful instances, and adjusting client request pool accordingly, would probably solve the problem to some extent. However, with a steadily increasing traffic, it would be just a matter of time before hitting the limits again.

Another solution I looked into was isolating points of access between external providers. Instead of using common thread pool for all requests to downstream services, introduce separate HTTP client for each service. This way, each HTTP client in Order Service would have its own thread pool and a custom timeout value, configured to match individual performance characteristics of a service it talks to. With a client request pool size set way above the sizes of those individual downstream pools, one misbehaving external provider would not be able to take down the whole platform. Reaching the limit of open connections to a particular provider would just cause all additional user requests involving this provider to fail immediately, consuming at most a fixed amount of threads in the client request pool.

# Hystrix

Further reasearching the subject, I have discovered [Hystrix][hystrix-homepage]. It is a latency and fault tolerance library designed to isolate points of access to remote systems, stop cascading failure and enable resilience in distributed systems. It has been developed by [Netflix][netflix-github] among many other great libraries supporting development of microservices, like [Eureka][eureka], [Ribbon][ribbon] or [Asgard][asgard]. Hystrix is able to provide the same thread pool isolation as mentioned above, together with lots of other useful features.

{% highlight java %}
@Component
@DefaultProperties()
public class Payex {

    private Client client;

    @HystrixCommand(fallbackMethod = "getUserAgreementFallback",
            groupKey = "Payex")
    public Response getUserAgreement(UserAgreementRequest request) {
        return client.getUserAgreement(request);
    }

    public Response getUserAgreementFallback(UserAgreementRequest request) {
        return UserAgreement.empty(); // something useful
        // or throw exception here
    }
}
{% endhighlight %}




(easiest would be to scale up instances, increase number of threads, then we used approach with separate threadpools,  but eventually ended up using Hystrix as its more robust & powerful, elegabt...)

(mention that open circuit-breaker also helps the provider recover)

(bonus feature, collapsing requests)

[sla]: https://en.wikipedia.org/wiki/Service-level_agreement
[hystrix-homepage]: https://github.com/Netflix/Hystrix
[netflix-github]: https://github.com/Netflix
[eureka]: https://github.com/Netflix/eureka
[asgard]: https://github.com/Netflix/asgard
[ribbon]: https://github.com/Netflix/ribbon
[spinnaker]: http://www.spinnaker.io/